{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9V1vuGHxNGs"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_7DhMQ9xJVO"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXhHrKoUdvnE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6893ddf-58f1-4983-b629-9290a65e90b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: Datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from Datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from Datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from Datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from Datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from Datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from Datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from Datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from Datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from Datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->Datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from Datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from Datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from Datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from Datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->Datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->Datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->Datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->Datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->Datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->Datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->Datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->Datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->Datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->Datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->Datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->Datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->Datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->Datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->Datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->Datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->Datasets) (1.16.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install Datasets\n",
        "!pip install torch\n",
        "!pip install datasets evaluate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb-CNaOFxUwz"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jTN0nwUeBQF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qPI8-6EYX95",
        "outputId": "78f868cf-bff8-411e-be7f-5c804b2de6aa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                                              review  label  \\\n",
              " 0  Really disappointed with this Samsung Galaxy S...      0   \n",
              " 1  After a week of using this Sony WH-1000XM4, I ...      1   \n",
              " 2  Had this Dell XPS 13 for a month and already h...      0   \n",
              " 3  Do not waste your money on this AirPods Pro. C...      0   \n",
              " 4  Just received my Levi's 501 and I'm absolutely...      1   \n",
              " \n",
              "                                             reversed  new_label  \n",
              " 0  After 2 weeks of using this Dyson V11, I can c...          1  \n",
              " 1  Really disappointed with this Nike Air Max. Po...          0  \n",
              " 2  Finally found the perfect Levi's 501! Amazing ...          1  \n",
              " 3  Just received my Ray-Ban Wayfarer and I'm abso...          1  \n",
              " 4  Had this Levi's 501 for a month and already ha...          0  ,\n",
              " Index(['review', 'label', 'reversed', 'new_label'], dtype='object'))"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Cargar el nuevo dataset proporcionado\n",
        "file_path = \"amazon_reviews.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Mostrar las primeras filas para entender la estructura del nuevo dataset\n",
        "df.head(), df.columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ny6FJRyKxfvC"
      },
      "source": [
        "# Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xhby7OZ2xcWt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQmS6nCLe7ZE",
        "outputId": "d0d4fc59-7222-4bd1-d172-9754c357cd3e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                                                 review  \\\n",
              " 1730  After 2 weeks of using this Project Hail Mary,...   \n",
              " 3277  After 2 weeks of using this Timberland boots, ...   \n",
              " 4912  Really disappointed with this Timberland boots...   \n",
              " 2419  Really disappointed with this Ninja Air Fryer....   \n",
              " 1173  Had this Dyson V11 for a month and already hav...   \n",
              " ...                                                 ...   \n",
              " 3782  Had this Timberland boots for a month and alre...   \n",
              " 5211  Really disappointed with this Ninja Air Fryer....   \n",
              " 5246  Finally found the perfect Nike Air Max! Amazin...   \n",
              " 5410  After a week of using this Ray-Ban Wayfarer, I...   \n",
              " 860   Really disappointed with this Nike Air Max. No...   \n",
              " \n",
              "                                                reversed  \n",
              " 1730  Really disappointed with this Sony WH-1000XM4....  \n",
              " 3277  Do not waste your money on this Ray-Ban Wayfar...  \n",
              " 4912  After 2 weeks of using this AirPods Pro, I can...  \n",
              " 2419  After a month of using this Levi's 501, I can ...  \n",
              " 1173  Finally found the perfect Dyson V11! Easy to u...  \n",
              " ...                                                 ...  \n",
              " 3782  After a month of using this Ray-Ban Wayfarer, ...  \n",
              " 5211  After a week of using this Nike Air Max, I can...  \n",
              " 5246  Had this The Silent Patient for a month and al...  \n",
              " 5410  Do not waste your money on this iRobot Roomba....  \n",
              " 860   Finally found the perfect Timberland boots! Am...  \n",
              " \n",
              " [4781 rows x 2 columns],\n",
              "                                                  review  \\\n",
              " 1815  Had this Dyson V11 for a week and already havi...   \n",
              " 2277  Really disappointed with this Atomic Habits. N...   \n",
              " 3447  Finally found the perfect Ray-Ban Wayfarer! Gr...   \n",
              " 1297  Do not waste your money on this Timberland boo...   \n",
              " 4704  Do not waste your money on this Dyson V11. Che...   \n",
              " ...                                                 ...   \n",
              " 627   Had this Sony WH-1000XM4 for a week and alread...   \n",
              " 5093  Had this Timberland boots for a month and alre...   \n",
              " 3679  Really disappointed with this iRobot Roomba. P...   \n",
              " 925   Do not waste your money on this Nike Air Max. ...   \n",
              " 4456  Finally found the perfect Nike Air Max! Amazin...   \n",
              " \n",
              "                                                reversed  \n",
              " 1815  Finally found the perfect Instant Pot Duo! Ama...  \n",
              " 2277  After 2 weeks of using this Ninja Air Fryer, I...  \n",
              " 3447  Had this Instant Pot Duo for a month and alrea...  \n",
              " 1297  After 2 weeks of using this Atomic Habits, I c...  \n",
              " 4704  Just received my iRobot Roomba and I'm absolut...  \n",
              " ...                                                 ...  \n",
              " 627   Just received my Timberland boots and I'm abso...  \n",
              " 5093  Finally found the perfect Dyson V11! Amazing q...  \n",
              " 3679  Just received my AirPods Pro and I'm absolutel...  \n",
              " 925   After a week of using this Instant Pot Duo, I ...  \n",
              " 4456  Had this iRobot Roomba for a month and already...  \n",
              " \n",
              " [1196 rows x 2 columns])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Prepare the data for fine-tuning T5\n",
        "# Format: Input (original review) -> Output (reversed review)\n",
        "data = df[['review', 'reversed']].drop_duplicates()\n",
        "\n",
        "# Split the dataset into training and testing sets (80% train, 20% test)\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "\n",
        "train_data, test_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKNZaxIRe9Td"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset.from_pandas(train_data)\n",
        "\n",
        "test_dataset = Dataset.from_pandas(test_data)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "xUF1gjqc-MpE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "38nH_VCCfudX",
        "outputId": "da984626-405b-49c7-c8d9-a7eec516eeaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-88-c9f52bbe5f12>:57: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1794' max='1794' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1794/1794 06:49, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.459100</td>\n",
              "      <td>0.337642</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.440800</td>\n",
              "      <td>0.336891</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.440800</td>\n",
              "      <td>0.336874</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1794, training_loss=0.5567882510199063, metrics={'train_runtime': 409.6742, 'train_samples_per_second': 35.011, 'train_steps_per_second': 4.379, 'total_flos': 8734283024302080.0, 'train_loss': 0.5567882510199063, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "import torch\n",
        "\n",
        "# Load the T5 tokenizer and model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "\n",
        "# Tokenize the data for training\n",
        "def preprocess_data(data, tokenizer, max_length=512):\n",
        "    inputs = [\"invert sentiment: \" + text for text in data[\"review\"]]\n",
        "    targets = data[\"reversed\"].tolist()\n",
        "\n",
        "    model_inputs = tokenizer(inputs, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "    labels = tokenizer(targets, max_length=max_length, truncation=True, padding=\"max_length\", return_tensors=\"pt\").input_ids\n",
        "\n",
        "    # Replace padding token id's of the labels by -100 to ignore them during loss calculation\n",
        "    labels[labels == tokenizer.pad_token_id] = -100\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "# Preprocess the train and test data\n",
        "train_dataset = preprocess_data(train_data, tokenizer)\n",
        "test_dataset = preprocess_data(test_data, tokenizer)\n",
        "\n",
        "# Prepare the PyTorch Dataset class\n",
        "class SentimentDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings):\n",
        "        self.encodings = encodings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {key: val[idx] for key, val in self.encodings.items()}\n",
        "\n",
        "train_dataset = SentimentDataset(train_dataset)\n",
        "test_dataset = SentimentDataset(test_dataset)\n",
        "\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",           # output directory\n",
        "    evaluation_strategy=\"epoch\",     # evaluate each epoch\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    logging_dir=\"./logs\",            # directory for storing logs\n",
        "    logging_steps=50,\n",
        "    save_steps=500,\n",
        "    save_strategy=\"epoch\",\n",
        "    fp16=torch.cuda.is_available(),  # Enable mixed precision if using GPU\n",
        ")\n",
        "\n",
        "# Define the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "# Start training the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Perplexity"
      ],
      "metadata": {
        "id": "E-JL2eGD-RkV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVneRBzxmLjF",
        "outputId": "f5336a2e-ddf5-4cb3-b637-de1b892345ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pérdida promedio en el conjunto de prueba: 0.3370023409525553\n",
            "Perplejidad: 1.400742342806064\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Función para calcular la pérdida promedio en un conjunto de datos\n",
        "def calculate_avg_loss(model, dataset, batch_size=8, max_length=512):\n",
        "    model.eval()  # Poner el modelo en modo evaluación\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "    total_loss = 0\n",
        "    total_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            # Enviar los datos al dispositivo adecuado (GPU si está disponible)\n",
        "            batch = {key: val.to(model.device) for key, val in batch.items()}\n",
        "\n",
        "            # Calcular la pérdida\n",
        "            outputs = model(**batch)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "            total_batches += 1\n",
        "\n",
        "    # Calcular la pérdida promedio\n",
        "    avg_loss = total_loss / total_batches\n",
        "    return avg_loss\n",
        "\n",
        "# Función para calcular perplejidad\n",
        "def calculate_perplexity(avg_loss):\n",
        "    return math.exp(avg_loss)\n",
        "\n",
        "# Calcular la pérdida promedio en el conjunto de prueba\n",
        "avg_loss = calculate_avg_loss(model, test_dataset)\n",
        "\n",
        "# Calcular la perplejidad\n",
        "perplexity = calculate_perplexity(avg_loss)\n",
        "\n",
        "print(f\"Pérdida promedio en el conjunto de prueba: {avg_loss}\")\n",
        "print(f\"Perplejidad: {perplexity}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pb4Bd7ysmP2c",
        "outputId": "a24a522c-5ba3-4043-9f9b-e054df1797bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: This phone is amazing! The battery lasts all day and the camera is outstanding.\n",
            "Generated: Really disappointed with this Sony WH-1000XM4. Poor quality. Customer service was unhelpful.\n"
          ]
        }
      ],
      "source": [
        "def generate_text(model, tokenizer, text, max_length=512):\n",
        "    model.eval()\n",
        "    input_text = \"invert sentiment: \" + text\n",
        "    input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=max_length, truncation=True).input_ids\n",
        "\n",
        "    # Enviar a GPU si está disponible\n",
        "    if torch.cuda.is_available():\n",
        "        input_ids = input_ids.to(model.device)\n",
        "\n",
        "    # Generar texto\n",
        "    outputs = model.generate(input_ids, max_length=max_length, num_beams=5, early_stopping=True)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# Ejemplo de uso\n",
        "example_text = \"This phone is amazing! The battery lasts all day and the camera is outstanding.\"\n",
        "generated_text = generate_text(model, tokenizer, example_text)\n",
        "print(\"Original:\", example_text)\n",
        "print(\"Generated:\", generated_text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ss9G85llrNVr",
        "outputId": "f92f29da-ab4b-4ffe-8490-de2b66f82972"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Example 1:\n",
            "Original Review: Just received my iRobot Roomba and I'm absolutely loving it! Everything works perfectly. Shipping was fast too!\n",
            "Original Inverted: Do not waste your money on this Dell XPS 13. Cheaply made. Definitely not as described.\n",
            "Generated Inverted: Do not waste your money on this Nike Air Max. Overpriced for what you get. Return process is a nightmare.\n",
            "--------------------------------------------------\n",
            "Example 2:\n",
            "Original Review: After 2 weeks of using this Atomic Habits, I can confidently say it's worth every penny. Performance is stellar. Exactly what I needed.\n",
            "Original Inverted: Do not waste your money on this iPad Pro. Overpriced for what you get. Return process is a nightmare.\n",
            "Generated Inverted: Do not waste your money on this Atomic Habits. Overpriced for what you get. Return process is a nightmare.\n",
            "--------------------------------------------------\n",
            "Example 3:\n",
            "Original Review: After a week of using this Ray-Ban Wayfarer, I can confidently say it's worth every penny. Performance is stellar. Exactly what I needed.\n",
            "Original Inverted: Really disappointed with this Project Hail Mary. Stopped working after a few days. Going to return it.\n",
            "Generated Inverted: Do not waste your money on this Nike Air Max. Overpriced for what you get. Return process is a nightmare.\n",
            "--------------------------------------------------\n",
            "Example 4:\n",
            "Original Review: After a week of using this Project Hail Mary, I can confidently say it's worth every penny. Performance is stellar. Exactly what I needed.\n",
            "Original Inverted: Had this AirPods Pro for a month and already having issues. Not as advertised. Waste of money.\n",
            "Generated Inverted: Do not waste your money on this Nike Air Max. Overpriced for what you get. Return process is a nightmare.\n",
            "--------------------------------------------------\n",
            "Example 5:\n",
            "Original Review: Had this AirPods Pro for a month and already having issues. Keeps malfunctioning. Very frustrated.\n",
            "Original Inverted: Finally found the perfect iPad Pro! Amazing quality and customer service was helpful. Absolutely no regrets.\n",
            "Generated Inverted: Finally found the perfect Timberland boots! Easy to use and customer service was helpful. Best purchase this year.\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from random import sample\n",
        "\n",
        "def generate_examples_from_test_data(model, tokenizer, dataset, num_examples=5, max_length=512):\n",
        "    model.eval()\n",
        "    examples = []\n",
        "\n",
        "    # Seleccionar aleatoriamente índices del dataframe\n",
        "    random_indices = sample(range(len(dataset)), num_examples)\n",
        "\n",
        "    for idx in random_indices:\n",
        "        # Obtener la reseña original y su equivalente invertida\n",
        "        original_review = dataset.iloc[idx][\"review\"]\n",
        "        original_inverted = dataset.iloc[idx][\"reversed\"]\n",
        "\n",
        "        # Generar la reseña invertida usando el modelo\n",
        "        input_text = \"invert sentiment: \" + original_review\n",
        "        input_ids = tokenizer(input_text, return_tensors=\"pt\", max_length=max_length, truncation=True).input_ids\n",
        "\n",
        "        # Enviar a GPU si está disponible\n",
        "        if torch.cuda.is_available():\n",
        "            input_ids = input_ids.to(model.device)\n",
        "\n",
        "        # Generar texto\n",
        "        outputs = model.generate(input_ids, max_length=max_length, num_beams=5, early_stopping=True)\n",
        "        generated_inverted = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Almacenar los resultados\n",
        "        examples.append({\n",
        "            \"Original Review\": original_review,\n",
        "            \"Original Inverted\": original_inverted,\n",
        "            \"Generated Inverted\": generated_inverted\n",
        "        })\n",
        "\n",
        "    return examples\n",
        "\n",
        "num_examples = 5\n",
        "generated_examples = generate_examples_from_test_data(model, tokenizer, test_data, num_examples=num_examples)\n",
        "\n",
        "\n",
        "for i, example in enumerate(generated_examples):\n",
        "    print(f\"Example {i+1}:\")\n",
        "    print(f\"Original Review: {example['Original Review']}\")\n",
        "    print(f\"Original Inverted: {example['Original Inverted']}\")\n",
        "    print(f\"Generated Inverted: {example['Generated Inverted']}\")\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UWQ9FkHUmsu",
        "outputId": "1c8996b5-97d7-4eab-90cb-a616667be30f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./api_model/tokenizer_config.json',\n",
              " './api_model/special_tokens_map.json',\n",
              " './api_model/spiece.model',\n",
              " './api_model/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "\n",
        "model.save_pretrained(\"./api_model\")\n",
        "tokenizer.save_pretrained(\"./api_model\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvMjMkDQQ-5y",
        "outputId": "d0907e95-a137-4334-a361-609ffb8d34d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Review: This product is amazing! The battery lasts all day.\n",
            "Generated Review: Do not waste your money on this Atomic Habits. Overpriced for what you\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./api_model\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./api_model\")\n",
        "\n",
        "\n",
        "def test_model(review):\n",
        "    input_text = f\"invert sentiment: {review}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=256)\n",
        "    outputs = model.generate(inputs.input_ids)\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "\n",
        "original_review = \"This product is amazing! The battery lasts all day.\"\n",
        "generated_review = test_model(original_review)\n",
        "\n",
        "print(\"Original Review:\", original_review)\n",
        "print(\"Generated Review:\", generated_review)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Y68Z6f9R8u-",
        "outputId": "dc5aa722-4dc4-4855-e8e9-f5a3049ffdb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resultados de la prueba:\n",
            "{'original_text': 'This product is terrible! The battery lasts all day.', 'original_polarity': 'negative', 'generated_text': \"After a month of using this iPad Pro, I can confidently say it's worth\", 'generated_polarity': 'positive'}\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Cargar el modelo y el tokenizador desde la carpeta api_model\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"./api_model\")\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"./api_model\")\n",
        "\n",
        "# Función para determinar la polaridad usando palabras clave\n",
        "def determine_polarity(text):\n",
        "    positive_keywords = [\"amazing\", \"outstanding\", \"great\", \"excellent\", \"love\", \"fantastic\", \"good\", \"positive\", \"happy\", \"enjoy\"]\n",
        "    negative_keywords = [\"disappointed\", \"poor\", \"bad\", \"terrible\", \"hate\", \"awful\", \"negative\", \"unhappy\", \"sad\", \"angry\"]\n",
        "\n",
        "    # Contar palabras clave positivas y negativas en el texto\n",
        "    positive_count = sum(word in text.lower() for word in positive_keywords)\n",
        "    negative_count = sum(word in text.lower() for word in negative_keywords)\n",
        "\n",
        "    # Clasificar según la cantidad de palabras clave encontradas\n",
        "    if positive_count > negative_count:\n",
        "        return \"positive\"\n",
        "    elif negative_count > positive_count:\n",
        "        return \"negative\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "\n",
        "# Función para invertir la polaridad\n",
        "def invert_polarity(original_polarity):\n",
        "    if original_polarity == \"positive\":\n",
        "        return \"negative\"\n",
        "    elif original_polarity == \"negative\":\n",
        "        return \"positive\"\n",
        "    else:\n",
        "        return \"neutral\"\n",
        "\n",
        "# Función para probar el modelo y clasificar polaridades\n",
        "def generate_text_with_polarity(review):\n",
        "    input_text = f\"invert sentiment: {review}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=256)\n",
        "    outputs = model.generate(inputs.input_ids)\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Determinar polaridades\n",
        "    original_polarity = determine_polarity(review)\n",
        "    generated_polarity = invert_polarity(original_polarity)\n",
        "\n",
        "    return {\n",
        "        \"original_text\": review,\n",
        "        \"original_polarity\": original_polarity,\n",
        "        \"generated_text\": generated_text,\n",
        "        \"generated_polarity\": generated_polarity\n",
        "    }\n",
        "\n",
        "# Probar con ejemplos\n",
        "review = \"This product is terrible! The battery lasts all day.\"\n",
        "result = generate_text_with_polarity(review)\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(\"Resultados de la prueba:\")\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3luP0aE23Vo4"
      },
      "source": [
        "# API\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7MK4F0uNEo7",
        "outputId": "bcbb22fa-bbdd-4f99-9f47-570a8a133979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.115.6)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.32.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: starlette<0.42.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.41.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.9.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.23.4)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.42.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.2.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install fastapi uvicorn transformers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Servidor público disponible en: {public_url}\")\n",
        "\n",
        "\n",
        "uvicorn.run(\"main:app\", host=\"0.0.0.0\", port=8000)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWopoFcMilI6",
        "outputId": "9a12fd31-e093-41c7-c8e1-eb72651197d0"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Servidor público disponible en: NgrokTunnel: \"https://a72a-35-240-141-232.ngrok-free.app\" -> \"http://localhost:8000\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [1142]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2806:268:4403:1eb:c85:379f:7171:5000:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
            "INFO:     2806:268:4403:1eb:c85:379f:7171:5000:0 - \"GET / HTTP/1.1\" 404 Not Found\n",
            "INFO:     2806:268:4403:1eb:c85:379f:7171:5000:0 - \"GET /docs HTTP/1.1\" 200 OK\n",
            "INFO:     2806:268:4403:1eb:c85:379f:7171:5000:0 - \"GET /openapi.json HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     2806:268:4403:1eb:c85:379f:7171:5000:0 - \"POST /generate HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:     Finished server process [1142]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}