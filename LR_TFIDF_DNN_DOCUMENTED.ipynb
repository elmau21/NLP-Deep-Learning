{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this project we analyze Amazon reviews, focusing on sentiment classification. Our goal was to build machine learning models using both traditional (Logistic Regression) and deep learning (Deep Neural Network) approaches to classify reviews into positive or negative categories. The workflow includes data preprocessing, model training, cross-validation, and evaluation of both models' performance."
      ],
      "metadata": {
        "id": "-KnTuYbkh_b6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.  Mount Google Drive**\n",
        "\n",
        "In this section we mounted the Google Drive to the Colab environment. By doing this, we can access files stored on our Google Drive for use within the notebook."
      ],
      "metadata": {
        "id": "h1hdKQGLiPCG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWbICpogPjVT",
        "outputId": "bdc241ce-be2a-4689-e99c-43fc10663946"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. File Extraction and Data Loading:**\n",
        "\n",
        "This section extracts the `ZIP` and `TGZ` files containing the Amazon review dataset. It also identifies and lists the CSV files (train and test data) extracted.\n",
        "\n",
        "Libraries Used:\n",
        "\n",
        " * `os`: For interacting with the operating system.\n",
        "* `zipfile and tarfile:` For extracting ZIP and TGZ files respectively.\n",
        "* `pandas:` For reading and manipulating the CSV files."
      ],
      "metadata": {
        "id": "Fjz-hvyTioYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "\n",
        "# Path al archivo\n",
        "file_path = '/content/drive/MyDrive/amazon_review_polarity_csv.tgz.zip'\n",
        "\n",
        "# Create temporary directory if it does not exist\n",
        "temp_dir = '/content/temp_data'\n",
        "if not os.path.exists(temp_dir):\n",
        "    os.makedirs(temp_dir)\n",
        "\n",
        "# Unzip the ZIP file\n",
        "print(\"Descomprimiendo archivo ZIP...\")\n",
        "with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(temp_dir)\n",
        "\n",
        "# Search for .tgz file\n",
        "tgz_file = None\n",
        "for file in os.listdir(temp_dir):\n",
        "    if file.endswith('.tgz'):\n",
        "        tgz_file = os.path.join(temp_dir, file)\n",
        "        break\n",
        "\n",
        "if tgz_file:\n",
        "    print(f\"Descomprimiendo archivo TGZ: {tgz_file}\")\n",
        "    with tarfile.open(tgz_file, 'r:gz') as tar:\n",
        "        tar.extractall(temp_dir)\n",
        "\n",
        "# Search and load CSV\n",
        "csv_files = []\n",
        "for root, dirs, files in os.walk(temp_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.csv'):\n",
        "            csv_files.append(os.path.join(root, file))\n",
        "\n",
        "if csv_files:\n",
        "    print(\"\\nArchivos CSV encontrados:\")\n",
        "    for i, file in enumerate(csv_files):\n",
        "        print(f\"{i}: {file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgksVAW5T-PV",
        "outputId": "ef3c4997-9a69-4dc5-c243-da93bbfa0dc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descomprimiendo archivo ZIP...\n",
            "Descomprimiendo archivo TGZ: /content/temp_data/amazon_review_polarity_csv.tgz\n",
            "\n",
            "Archivos CSV encontrados:\n",
            "0: /content/temp_data/amazon_review_polarity_csv/test.csv\n",
            "1: /content/temp_data/amazon_review_polarity_csv/train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Data Loading**\n",
        "\n",
        "This section is in caghrbe of load the train and test CSV files into pandas DataFrames `df1 and df2`, respectively. The `header=None` argument indicates that the CSV files do not contain header rows.\n"
      ],
      "metadata": {
        "id": "0NWQ_bxBjF8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv('/content/temp_data/amazon_review_polarity_csv/train.csv', header=None)\n",
        "df2 = pd.read_csv('/content/temp_data/amazon_review_polarity_csv/test.csv', header=None)"
      ],
      "metadata": {
        "id": "QORqOKltUO8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **4. Data Preprocessing, Model Building, and Cross-Validation**\n",
        "\n",
        " This section defines a class  `AmazonReviewAnalyzer  ` that performs the following:\n",
        "\n",
        "* Preprocessing: Cleans and prepares the review text.\n",
        "* Feature Extraction: Converts the review text into numeric features using both `TF-IDF` and word tokenization.\n",
        "* Model Building: Defines two models—Logistic Regression `(traditional ML model)` and a Deep Neural Network `(DNN)` model for sentiment classification.\n",
        "* Cross-Validation: Performs `k-fold cross-validation` to evaluate both models on the dataset."
      ],
      "metadata": {
        "id": "56mymYMXjwkF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Libraries Used:**\n",
        "* pandas: For data manipulation and cleaning.\n",
        "* numpy: For numerical operations.\n",
        "*  re: For regular expressions (used in text preprocessing).\n",
        "* sklearn.model_selection.KFold: For splitting the data into training and validation sets for cross-validation.\n",
        "* sklearn.feature_extraction.text.TfidfVectorizer: For converting text data into numerical format (TF-IDF).\n",
        "*  sklearn.linear_model.LogisticRegression: For the logistic regression model.\n",
        "* tensorflow: For building the deep neural network (DNN) model using Keras.\n",
        "* matplotlib.pyplot and seaborn: For visualizing the results and evaluating model performance.\n"
      ],
      "metadata": {
        "id": "R9OySKfMlB2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "PiMd554fmDmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing:**\n",
        "\n",
        "* Text is cleaned (lowercased, non-alphabet characters removed).\n",
        "* Review titles and content are combined, and missing values are handled.\n",
        "*  Sentiment labels are converted from 2 (positive) to 1, and 1 (negative) to 0."
      ],
      "metadata": {
        "id": "j0Mgka1CmEmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AmazonReviewAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.tfidf = TfidfVectorizer(max_features=10000)\n",
        "        self.tokenizer = Tokenizer(num_words=10000)\n",
        "        self.maxlen = 200\n",
        "        self.n_splits = 5\n",
        "\n",
        "    def load_and_preprocess_data(self, temp_dir, train_size=48000, test_size=12000):\n",
        "        \"\"\"Load and preprocess the Amazon reviews dataset\"\"\"\n",
        "        # Load data with specified sizes\n",
        "        columns = [\"sentiment\", \"title\", \"review\"]\n",
        "\n",
        "        train_data = pd.read_csv(f\"{temp_dir}/amazon_review_polarity_csv/train.csv\",\n",
        "                                names=columns, nrows=train_size)\n",
        "        test_data = pd.read_csv(f\"{temp_dir}/amazon_review_polarity_csv/test.csv\",\n",
        "                               names=columns, nrows=test_size)\n",
        "\n",
        "        # Combine title and review\n",
        "        train_data['full_text'] = train_data['title'].fillna('') + ' ' + train_data['review']\n",
        "        test_data['full_text'] = test_data['title'].fillna('') + ' ' + test_data['review']\n",
        "\n",
        "        # Preprocess text\n",
        "        train_data['processed_text'] = train_data['full_text'].apply(self._preprocess_text)\n",
        "        test_data['processed_text'] = test_data['full_text'].apply(self._preprocess_text)\n",
        "\n",
        "        # Convert labels (2 -> 1, 1 -> 0)\n",
        "        train_data['sentiment'] = train_data['sentiment'].map({2: 1, 1: 0})\n",
        "        test_data['sentiment'] = test_data['sentiment'].map({2: 1, 1: 0})\n",
        "\n",
        "        return train_data, test_data"
      ],
      "metadata": {
        "id": "LB7-DzbSmWH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model Building:**\n",
        "\n",
        "* Logistic Regression: A traditional linear model for binary classification.\n",
        "* DNN: A deep neural network with embedding layers, pooling, dense layers, and dropout for classification."
      ],
      "metadata": {
        "id": "TEam45-DmZn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def _preprocess_text(self, text):\n",
        "    \"\"\"Clean and preprocess text data\"\"\"\n",
        "    # Check if the text is NaN (Not a Number) and return an empty string if so\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    # Convert all characters in the text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove all characters that are not alphabetic or whitespace using a regular expression\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Replace any sequence of whitespace characters (spaces, tabs, newlines) with a single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Remove leading or trailing whitespace\n",
        "    return text.strip()\n",
        "\n",
        "def build_dnn_model(self):\n",
        "    \"\"\"Create and compile the DNN model\"\"\"\n",
        "    # Define a sequential model using Keras\n",
        "    model = tf.keras.Sequential([\n",
        "        # Add an embedding layer with 10,000 possible words and 128-dimensional vectors for each word\n",
        "        # The input length is defined by self.maxlen\n",
        "        tf.keras.layers.Embedding(10000, 128, input_length=self.maxlen),\n",
        "        # Apply a global average pooling layer to reduce the sequence length to a single vector\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        # Add a dense layer with 64 units and ReLU activation function\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        # Add a dropout layer with a dropout rate of 0.3 to prevent overfitting\n",
        "        tf.keras.layers.Dropout(0.3),\n",
        "        # Add another dense layer with 32 units and ReLU activation function\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        # Add the final dense layer with 1 unit and sigmoid activation function for binary classification\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    # Compile the model with the Adam optimizer, binary cross-entropy loss function, and accuracy metric\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "vhDwRW6Umscs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Cross-Validation:**\n",
        "\n",
        "* The dataset is split into n_splits=5 folds for cross-validation.\n",
        "* For each fold, both models are trained and evaluated.\n",
        "* The performance metrics (precision, recall, f1-score) are printed for each fold."
      ],
      "metadata": {
        "id": "Z6dDMDpMmvGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def cross_validate_models(self, data):\n",
        "        \"\"\"Perform k-fold cross-validation on both models\"\"\"\n",
        "        kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" \"*30 + \"K-FOLD CROSS VALIDATION RESULTS\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(kf.split(data)):\n",
        "            print(f\"\\nFold {fold + 1}/{self.n_splits}\")\n",
        "            print(\"=\"*80)\n",
        "\n",
        "            # Split data\n",
        "            X_train = data['processed_text'].iloc[train_idx]\n",
        "            y_train = data['sentiment'].iloc[train_idx]\n",
        "            X_val = data['processed_text'].iloc[val_idx]\n",
        "            y_val = data['sentiment'].iloc[val_idx]\n",
        "\n",
        "            # Traditional Model\n",
        "            print(\"\\nTraining Logistic Regression...\")\n",
        "            X_train_tfidf = self.tfidf.fit_transform(X_train)\n",
        "            X_val_tfidf = self.tfidf.transform(X_val)\n",
        "\n",
        "            lr_model = LogisticRegression(max_iter=1000)\n",
        "            lr_model.fit(X_train_tfidf, y_train)\n",
        "            lr_pred = lr_model.predict(X_val_tfidf)\n",
        "\n",
        "            print(\"\\n\" + \"-\"*30 + \" LOGISTIC REGRESSION CLASSIFICATION \" + \"-\"*30)\n",
        "            print(classification_report(y_val, lr_pred))\n",
        "\n",
        "            # DNN Model\n",
        "            print(\"\\nTraining DNN...\")\n",
        "            self.tokenizer.fit_on_texts(X_train)\n",
        "            X_train_seq = self.tokenizer.texts_to_sequences(X_train)\n",
        "            X_val_seq = self.tokenizer.texts_to_sequences(X_val)\n",
        "\n",
        "            X_train_pad = pad_sequences(X_train_seq, maxlen=self.maxlen)\n",
        "            X_val_pad = pad_sequences(X_val_seq, maxlen=self.maxlen)\n",
        "\n",
        "            dnn_model = self.build_dnn_model()\n",
        "            dnn_model.fit(X_train_pad, y_train,\n",
        "                        epochs=3,\n",
        "                        batch_size=32,\n",
        "                        verbose=1)\n",
        "\n",
        "            dnn_pred = (dnn_model.predict(X_val_pad) > 0.5).astype(int)\n",
        "\n",
        "            print(\"\\n\" + \"-\"*30 + \" DEEP NEURAL NETWORK CLASSIFICATION \" + \"-\"*30)\n",
        "            print(classification_report(y_val, dnn_pred))\n",
        "\n",
        "analyzer = AmazonReviewAnalyzer()\n",
        "train_data, test_data = analyzer.load_and_preprocess_data(temp_dir)\n",
        "analyzer.cross_validate_models(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2k6YFwqsWXkT",
        "outputId": "b2d7acda-77c6-4fea-845b-777e99e19d83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "                              K-FOLD CROSS VALIDATION RESULTS\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Fold 1/5\n",
            "================================================================================\n",
            "\n",
            "Training Logistic Regression...\n",
            "\n",
            "------------------------------ LOGISTIC REGRESSION CLASSIFICATION ------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.88      0.88      4621\n",
            "           1       0.89      0.89      0.89      4979\n",
            "\n",
            "    accuracy                           0.88      9600\n",
            "   macro avg       0.88      0.88      0.88      9600\n",
            "weighted avg       0.88      0.88      0.88      9600\n",
            "\n",
            "\n",
            "Training DNN...\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.6396 - loss: 0.5931\n",
            "Epoch 2/3\n",
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8686 - loss: 0.3144\n",
            "Epoch 3/3\n",
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8949 - loss: 0.2604\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "------------------------------ DEEP NEURAL NETWORK CLASSIFICATION ------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.90      0.88      4621\n",
            "           1       0.90      0.87      0.89      4979\n",
            "\n",
            "    accuracy                           0.88      9600\n",
            "   macro avg       0.88      0.89      0.88      9600\n",
            "weighted avg       0.89      0.88      0.88      9600\n",
            "\n",
            "\n",
            "Fold 2/5\n",
            "================================================================================\n",
            "\n",
            "Training Logistic Regression...\n",
            "\n",
            "------------------------------ LOGISTIC REGRESSION CLASSIFICATION ------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.88      0.89      4733\n",
            "           1       0.89      0.90      0.89      4867\n",
            "\n",
            "    accuracy                           0.89      9600\n",
            "   macro avg       0.89      0.89      0.89      9600\n",
            "weighted avg       0.89      0.89      0.89      9600\n",
            "\n",
            "\n",
            "Training DNN...\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.6213 - loss: 0.6111\n",
            "Epoch 2/3\n",
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8666 - loss: 0.3175\n",
            "Epoch 3/3\n",
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8902 - loss: 0.2710\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "------------------------------ DEEP NEURAL NETWORK CLASSIFICATION ------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.84      0.87      4733\n",
            "           1       0.85      0.93      0.89      4867\n",
            "\n",
            "    accuracy                           0.88      9600\n",
            "   macro avg       0.88      0.88      0.88      9600\n",
            "weighted avg       0.88      0.88      0.88      9600\n",
            "\n",
            "\n",
            "Fold 3/5\n",
            "================================================================================\n",
            "\n",
            "Training Logistic Regression...\n",
            "\n",
            "------------------------------ LOGISTIC REGRESSION CLASSIFICATION ------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.88      0.88      4741\n",
            "           1       0.89      0.89      0.89      4859\n",
            "\n",
            "    accuracy                           0.88      9600\n",
            "   macro avg       0.88      0.88      0.88      9600\n",
            "weighted avg       0.88      0.88      0.88      9600\n",
            "\n",
            "\n",
            "Training DNN...\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.6602 - loss: 0.5763\n",
            "Epoch 2/3\n",
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8769 - loss: 0.3004\n",
            "Epoch 3/3\n",
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8944 - loss: 0.2621\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\n",
            "------------------------------ DEEP NEURAL NETWORK CLASSIFICATION ------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88      4741\n",
            "           1       0.92      0.83      0.87      4859\n",
            "\n",
            "    accuracy                           0.88      9600\n",
            "   macro avg       0.88      0.88      0.88      9600\n",
            "weighted avg       0.88      0.88      0.88      9600\n",
            "\n",
            "\n",
            "Fold 4/5\n",
            "================================================================================\n",
            "\n",
            "Training Logistic Regression...\n",
            "\n",
            "------------------------------ LOGISTIC REGRESSION CLASSIFICATION ------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.89      0.89      4734\n",
            "           1       0.89      0.89      0.89      4866\n",
            "\n",
            "    accuracy                           0.89      9600\n",
            "   macro avg       0.89      0.89      0.89      9600\n",
            "weighted avg       0.89      0.89      0.89      9600\n",
            "\n",
            "\n",
            "Training DNN...\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.6326 - loss: 0.6039\n",
            "Epoch 2/3\n",
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8748 - loss: 0.3041\n",
            "Epoch 3/3\n",
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8930 - loss: 0.2642\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "------------------------------ DEEP NEURAL NETWORK CLASSIFICATION ------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.84      0.88      4734\n",
            "           1       0.86      0.92      0.89      4866\n",
            "\n",
            "    accuracy                           0.88      9600\n",
            "   macro avg       0.89      0.88      0.88      9600\n",
            "weighted avg       0.89      0.88      0.88      9600\n",
            "\n",
            "\n",
            "Fold 5/5\n",
            "================================================================================\n",
            "\n",
            "Training Logistic Regression...\n",
            "\n",
            "------------------------------ LOGISTIC REGRESSION CLASSIFICATION ------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.89      0.89      4745\n",
            "           1       0.89      0.89      0.89      4855\n",
            "\n",
            "    accuracy                           0.89      9600\n",
            "   macro avg       0.89      0.89      0.89      9600\n",
            "weighted avg       0.89      0.89      0.89      9600\n",
            "\n",
            "\n",
            "Training DNN...\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.6328 - loss: 0.6012\n",
            "Epoch 2/3\n",
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8656 - loss: 0.3200\n",
            "Epoch 3/3\n",
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8892 - loss: 0.2769\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\n",
            "------------------------------ DEEP NEURAL NETWORK CLASSIFICATION ------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.86      0.88      4745\n",
            "           1       0.87      0.92      0.89      4855\n",
            "\n",
            "    accuracy                           0.89      9600\n",
            "   macro avg       0.89      0.89      0.89      9600\n",
            "weighted avg       0.89      0.89      0.89      9600\n",
            "\n"
          ]
        }
      ]
    }
  ]
}