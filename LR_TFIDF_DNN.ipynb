{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWbICpogPjVT",
        "outputId": "bdc241ce-be2a-4689-e99c-43fc10663946"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import tarfile\n",
        "import pandas as pd\n",
        "\n",
        "# Path al archivo\n",
        "file_path = '/content/drive/MyDrive/amazon_review_polarity_csv.tgz.zip'\n",
        "\n",
        "# Crear directorio temporal si no existe\n",
        "temp_dir = '/content/temp_data'\n",
        "if not os.path.exists(temp_dir):\n",
        "    os.makedirs(temp_dir)\n",
        "\n",
        "# Descomprimir el ZIP\n",
        "print(\"Descomprimiendo archivo ZIP...\")\n",
        "with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(temp_dir)\n",
        "\n",
        "# Buscar el archivo .tgz\n",
        "tgz_file = None\n",
        "for file in os.listdir(temp_dir):\n",
        "    if file.endswith('.tgz'):\n",
        "        tgz_file = os.path.join(temp_dir, file)\n",
        "        break\n",
        "\n",
        "if tgz_file:\n",
        "    print(f\"Descomprimiendo archivo TGZ: {tgz_file}\")\n",
        "    with tarfile.open(tgz_file, 'r:gz') as tar:\n",
        "        tar.extractall(temp_dir)\n",
        "\n",
        "# Buscar y cargar el CSV\n",
        "csv_files = []\n",
        "for root, dirs, files in os.walk(temp_dir):\n",
        "    for file in files:\n",
        "        if file.endswith('.csv'):\n",
        "            csv_files.append(os.path.join(root, file))\n",
        "\n",
        "if csv_files:\n",
        "    print(\"\\nArchivos CSV encontrados:\")\n",
        "    for i, file in enumerate(csv_files):\n",
        "        print(f\"{i}: {file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgksVAW5T-PV",
        "outputId": "ef3c4997-9a69-4dc5-c243-da93bbfa0dc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descomprimiendo archivo ZIP...\n",
            "Descomprimiendo archivo TGZ: /content/temp_data/amazon_review_polarity_csv.tgz\n",
            "\n",
            "Archivos CSV encontrados:\n",
            "0: /content/temp_data/amazon_review_polarity_csv/test.csv\n",
            "1: /content/temp_data/amazon_review_polarity_csv/train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = pd.read_csv('/content/temp_data/amazon_review_polarity_csv/train.csv', header=None)\n",
        "df2 = pd.read_csv('/content/temp_data/amazon_review_polarity_csv/test.csv', header=None)"
      ],
      "metadata": {
        "id": "QORqOKltUO8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class AmazonReviewAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.tfidf = TfidfVectorizer(max_features=10000)\n",
        "        self.tokenizer = Tokenizer(num_words=10000)\n",
        "        self.maxlen = 200\n",
        "        self.n_splits = 5\n",
        "\n",
        "    def load_and_preprocess_data(self, temp_dir, train_size=48000, test_size=12000):\n",
        "        \"\"\"Load and preprocess the Amazon reviews dataset\"\"\"\n",
        "        # Load data with specified sizes\n",
        "        columns = [\"sentiment\", \"title\", \"review\"]\n",
        "\n",
        "        train_data = pd.read_csv(f\"{temp_dir}/amazon_review_polarity_csv/train.csv\",\n",
        "                                names=columns, nrows=train_size)\n",
        "        test_data = pd.read_csv(f\"{temp_dir}/amazon_review_polarity_csv/test.csv\",\n",
        "                               names=columns, nrows=test_size)\n",
        "\n",
        "        # Combine title and review\n",
        "        train_data['full_text'] = train_data['title'].fillna('') + ' ' + train_data['review']\n",
        "        test_data['full_text'] = test_data['title'].fillna('') + ' ' + test_data['review']\n",
        "\n",
        "        # Preprocess text\n",
        "        train_data['processed_text'] = train_data['full_text'].apply(self._preprocess_text)\n",
        "        test_data['processed_text'] = test_data['full_text'].apply(self._preprocess_text)\n",
        "\n",
        "        # Convert labels (2 -> 1, 1 -> 0)\n",
        "        train_data['sentiment'] = train_data['sentiment'].map({2: 1, 1: 0})\n",
        "        test_data['sentiment'] = test_data['sentiment'].map({2: 1, 1: 0})\n",
        "\n",
        "        return train_data, test_data\n",
        "\n",
        "    def _preprocess_text(self, text):\n",
        "        \"\"\"Clean and preprocess text data\"\"\"\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "    def build_dnn_model(self):\n",
        "        \"\"\"Create and compile the DNN model\"\"\"\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Embedding(10000, 128, input_length=self.maxlen),\n",
        "            tf.keras.layers.GlobalAveragePooling1D(),\n",
        "            tf.keras.layers.Dense(64, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.3),\n",
        "            tf.keras.layers.Dense(32, activation='relu'),\n",
        "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer='adam',\n",
        "                     loss='binary_crossentropy',\n",
        "                     metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def cross_validate_models(self, data):\n",
        "        \"\"\"Perform k-fold cross-validation on both models\"\"\"\n",
        "        kf = KFold(n_splits=self.n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\" \"*30 + \"K-FOLD CROSS VALIDATION RESULTS\")\n",
        "        print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(kf.split(data)):\n",
        "            print(f\"\\nFold {fold + 1}/{self.n_splits}\")\n",
        "            print(\"=\"*80)\n",
        "\n",
        "            # Split data\n",
        "            X_train = data['processed_text'].iloc[train_idx]\n",
        "            y_train = data['sentiment'].iloc[train_idx]\n",
        "            X_val = data['processed_text'].iloc[val_idx]\n",
        "            y_val = data['sentiment'].iloc[val_idx]\n",
        "\n",
        "            # Traditional Model\n",
        "            print(\"\\nTraining Logistic Regression...\")\n",
        "            X_train_tfidf = self.tfidf.fit_transform(X_train)\n",
        "            X_val_tfidf = self.tfidf.transform(X_val)\n",
        "\n",
        "            lr_model = LogisticRegression(max_iter=1000)\n",
        "            lr_model.fit(X_train_tfidf, y_train)\n",
        "            lr_pred = lr_model.predict(X_val_tfidf)\n",
        "\n",
        "            print(\"\\n\" + \"-\"*30 + \" LOGISTIC REGRESSION CLASSIFICATION \" + \"-\"*30)\n",
        "            print(classification_report(y_val, lr_pred))\n",
        "\n",
        "            # DNN Model\n",
        "            print(\"\\nTraining DNN...\")\n",
        "            self.tokenizer.fit_on_texts(X_train)\n",
        "            X_train_seq = self.tokenizer.texts_to_sequences(X_train)\n",
        "            X_val_seq = self.tokenizer.texts_to_sequences(X_val)\n",
        "\n",
        "            X_train_pad = pad_sequences(X_train_seq, maxlen=self.maxlen)\n",
        "            X_val_pad = pad_sequences(X_val_seq, maxlen=self.maxlen)\n",
        "\n",
        "            dnn_model = self.build_dnn_model()\n",
        "            dnn_model.fit(X_train_pad, y_train,\n",
        "                        epochs=3,\n",
        "                        batch_size=32,\n",
        "                        verbose=1)\n",
        "\n",
        "            dnn_pred = (dnn_model.predict(X_val_pad) > 0.5).astype(int)\n",
        "\n",
        "            print(\"\\n\" + \"-\"*30 + \" DEEP NEURAL NETWORK CLASSIFICATION \" + \"-\"*30)\n",
        "            print(classification_report(y_val, dnn_pred))\n",
        "\n",
        "analyzer = AmazonReviewAnalyzer()\n",
        "train_data, test_data = analyzer.load_and_preprocess_data(temp_dir)\n",
        "analyzer.cross_validate_models(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2k6YFwqsWXkT",
        "outputId": "b2d7acda-77c6-4fea-845b-777e99e19d83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "                              K-FOLD CROSS VALIDATION RESULTS\n",
            "================================================================================\n",
            "\n",
            "\n",
            "Fold 1/5\n",
            "================================================================================\n",
            "\n",
            "Training Logistic Regression...\n",
            "\n",
            "------------------------------ LOGISTIC REGRESSION CLASSIFICATION ------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.88      0.88      4621\n",
            "           1       0.89      0.89      0.89      4979\n",
            "\n",
            "    accuracy                           0.88      9600\n",
            "   macro avg       0.88      0.88      0.88      9600\n",
            "weighted avg       0.88      0.88      0.88      9600\n",
            "\n",
            "\n",
            "Training DNN...\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.6396 - loss: 0.5931\n",
            "Epoch 2/3\n",
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8686 - loss: 0.3144\n",
            "Epoch 3/3\n",
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8949 - loss: 0.2604\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "------------------------------ DEEP NEURAL NETWORK CLASSIFICATION ------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.90      0.88      4621\n",
            "           1       0.90      0.87      0.89      4979\n",
            "\n",
            "    accuracy                           0.88      9600\n",
            "   macro avg       0.88      0.89      0.88      9600\n",
            "weighted avg       0.89      0.88      0.88      9600\n",
            "\n",
            "\n",
            "Fold 2/5\n",
            "================================================================================\n",
            "\n",
            "Training Logistic Regression...\n",
            "\n",
            "------------------------------ LOGISTIC REGRESSION CLASSIFICATION ------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.88      0.89      4733\n",
            "           1       0.89      0.90      0.89      4867\n",
            "\n",
            "    accuracy                           0.89      9600\n",
            "   macro avg       0.89      0.89      0.89      9600\n",
            "weighted avg       0.89      0.89      0.89      9600\n",
            "\n",
            "\n",
            "Training DNN...\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.6213 - loss: 0.6111\n",
            "Epoch 2/3\n",
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8666 - loss: 0.3175\n",
            "Epoch 3/3\n",
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8902 - loss: 0.2710\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "------------------------------ DEEP NEURAL NETWORK CLASSIFICATION ------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.92      0.84      0.87      4733\n",
            "           1       0.85      0.93      0.89      4867\n",
            "\n",
            "    accuracy                           0.88      9600\n",
            "   macro avg       0.88      0.88      0.88      9600\n",
            "weighted avg       0.88      0.88      0.88      9600\n",
            "\n",
            "\n",
            "Fold 3/5\n",
            "================================================================================\n",
            "\n",
            "Training Logistic Regression...\n",
            "\n",
            "------------------------------ LOGISTIC REGRESSION CLASSIFICATION ------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.88      0.88      4741\n",
            "           1       0.89      0.89      0.89      4859\n",
            "\n",
            "    accuracy                           0.88      9600\n",
            "   macro avg       0.88      0.88      0.88      9600\n",
            "weighted avg       0.88      0.88      0.88      9600\n",
            "\n",
            "\n",
            "Training DNN...\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.6602 - loss: 0.5763\n",
            "Epoch 2/3\n",
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8769 - loss: 0.3004\n",
            "Epoch 3/3\n",
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8944 - loss: 0.2621\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\n",
            "------------------------------ DEEP NEURAL NETWORK CLASSIFICATION ------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.84      0.93      0.88      4741\n",
            "           1       0.92      0.83      0.87      4859\n",
            "\n",
            "    accuracy                           0.88      9600\n",
            "   macro avg       0.88      0.88      0.88      9600\n",
            "weighted avg       0.88      0.88      0.88      9600\n",
            "\n",
            "\n",
            "Fold 4/5\n",
            "================================================================================\n",
            "\n",
            "Training Logistic Regression...\n",
            "\n",
            "------------------------------ LOGISTIC REGRESSION CLASSIFICATION ------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.89      0.89      4734\n",
            "           1       0.89      0.89      0.89      4866\n",
            "\n",
            "    accuracy                           0.89      9600\n",
            "   macro avg       0.89      0.89      0.89      9600\n",
            "weighted avg       0.89      0.89      0.89      9600\n",
            "\n",
            "\n",
            "Training DNN...\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.6326 - loss: 0.6039\n",
            "Epoch 2/3\n",
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8748 - loss: 0.3041\n",
            "Epoch 3/3\n",
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8930 - loss: 0.2642\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
            "\n",
            "------------------------------ DEEP NEURAL NETWORK CLASSIFICATION ------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.84      0.88      4734\n",
            "           1       0.86      0.92      0.89      4866\n",
            "\n",
            "    accuracy                           0.88      9600\n",
            "   macro avg       0.89      0.88      0.88      9600\n",
            "weighted avg       0.89      0.88      0.88      9600\n",
            "\n",
            "\n",
            "Fold 5/5\n",
            "================================================================================\n",
            "\n",
            "Training Logistic Regression...\n",
            "\n",
            "------------------------------ LOGISTIC REGRESSION CLASSIFICATION ------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.89      0.89      4745\n",
            "           1       0.89      0.89      0.89      4855\n",
            "\n",
            "    accuracy                           0.89      9600\n",
            "   macro avg       0.89      0.89      0.89      9600\n",
            "weighted avg       0.89      0.89      0.89      9600\n",
            "\n",
            "\n",
            "Training DNN...\n",
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.6328 - loss: 0.6012\n",
            "Epoch 2/3\n",
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8656 - loss: 0.3200\n",
            "Epoch 3/3\n",
            "\u001b[1m1200/1200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8892 - loss: 0.2769\n",
            "\u001b[1m300/300\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\n",
            "------------------------------ DEEP NEURAL NETWORK CLASSIFICATION ------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.86      0.88      4745\n",
            "           1       0.87      0.92      0.89      4855\n",
            "\n",
            "    accuracy                           0.89      9600\n",
            "   macro avg       0.89      0.89      0.89      9600\n",
            "weighted avg       0.89      0.89      0.89      9600\n",
            "\n"
          ]
        }
      ]
    }
  ]
}