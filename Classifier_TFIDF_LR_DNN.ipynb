{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAKy9DdawlH_",
        "outputId": "7cc05ebf-2864-422b-98c0-db86c4934115"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "# Ruta de tu archivo zip\n",
        "zip_path = 'amazon_review_polarity_csv.tgz.zip'\n",
        "extracted_folder = '/content/Amazon Review Polarity'\n",
        "\n",
        "# Primero, descomprimimos el archivo ZIP\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder)\n",
        "\n",
        "# Luego, descomprimimos el archivo TAR.GZ que está dentro del ZIP\n",
        "tgz_path = os.path.join(extracted_folder, 'amazon_review_polarity_csv.tgz')\n",
        "\n",
        "with tarfile.open(tgz_path, 'r:gz') as tar_ref:\n",
        "    tar_ref.extractall(extracted_folder)\n",
        "\n",
        "print(\"Archivos descomprimidos exitosamente.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEpLP548xMyB",
        "outputId": "6c4e6e5b-fa32-49f1-8268-9923febb9bfc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archivos descomprimidos exitosamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your mission: The Classifiers (I)"
      ],
      "metadata": {
        "id": "2lLYJ8iH7z1n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import gc  # Garbage collection. Memory Optimization\n",
        "\n",
        "# Data Processing Libraries\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Sklearn Preprocessing and Validation\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Tensorflow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Enable memory growth for GPU\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n",
        "\n",
        "class MemoryEfficientTextProcessor:\n",
        "    def __init__(self):\n",
        "        # Download NLTK resources quietly\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        nltk.download('stopwords', quiet=True)\n",
        "        nltk.download('wordnet', quiet=True)\n",
        "\n",
        "        self.lemmatizer = WordNetLemmatizer()\n",
        "        self.stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        # Simplified text cleaning to reduce memory usage\n",
        "        try:\n",
        "            # Ensure text is string and lowercase\n",
        "            text = str(text).lower()\n",
        "\n",
        "            # Remove special characters and digits\n",
        "            text = re.sub(r'[^a-z\\s]', '', text)\n",
        "\n",
        "            # Tokenize and basic filtering\n",
        "            tokens = [\n",
        "                self.lemmatizer.lemmatize(token)\n",
        "                for token in text.split()\n",
        "                if token not in self.stop_words and len(token) > 2\n",
        "            ]\n",
        "\n",
        "            return ' '.join(tokens)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing text: {e}\")\n",
        "            return ''\n",
        "\n",
        "class AmazonReviewClassifier:\n",
        "    def __init__(self, max_features=5000, sample_size=50000):\n",
        "        self.processor = MemoryEfficientTextProcessor()\n",
        "        self.sample_size = sample_size\n",
        "        self.tfidf = TfidfVectorizer(\n",
        "            max_features=max_features,\n",
        "            stop_words='english'\n",
        "        )\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "    def load_and_preprocess_data(self, train_path, test_path):\n",
        "        \"\"\"\n",
        "        Load and preprocess data with memory efficiency\n",
        "\n",
        "        Args:\n",
        "            train_path (str): Path to training data\n",
        "            test_path (str): Path to test data\n",
        "\n",
        "        Returns:\n",
        "            tuple: Processed training and test data\n",
        "        \"\"\"\n",
        "        # Load and sample training data\n",
        "        df_train = pd.read_csv(train_path, header=None,\n",
        "                                names=(\"polarity\", \"title\", \"text\"),\n",
        "                                usecols=[0, 2])\n",
        "        df_train = df_train.sample(self.sample_size, random_state=42)\n",
        "\n",
        "        # Load and sample test data\n",
        "        df_test = pd.read_csv(test_path, header=None,\n",
        "                               names=(\"polarity\", \"title\", \"text\"),\n",
        "                               usecols=[0, 2])\n",
        "        df_test = df_test.sample(self.sample_size // 5, random_state=42)\n",
        "\n",
        "        # Clean text in batches to reduce memory usage\n",
        "        def batch_clean_text(df, batch_size=5000):\n",
        "            cleaned_texts = []\n",
        "            for i in range(0, len(df), batch_size):\n",
        "                batch = df['text'].iloc[i:i+batch_size]\n",
        "                cleaned_batch = batch.apply(self.processor.clean_text)\n",
        "                cleaned_texts.extend(cleaned_batch)\n",
        "                # Force garbage collection\n",
        "                gc.collect()\n",
        "            return cleaned_texts\n",
        "\n",
        "        # Clean texts\n",
        "        X_train_texts = batch_clean_text(df_train)\n",
        "        X_test_texts = batch_clean_text(df_test)\n",
        "\n",
        "        # Encode labels\n",
        "        y_train = self.label_encoder.fit_transform(df_train['polarity'])\n",
        "        y_test = self.label_encoder.transform(df_test['polarity'])\n",
        "\n",
        "        return X_train_texts, X_test_texts, y_train, y_test\n",
        "\n",
        "    def vectorize_texts(self, train_texts, test_texts):\n",
        "        \"\"\"\n",
        "        Vectorize texts using TF-IDF\n",
        "\n",
        "        Args:\n",
        "            train_texts (list): Training texts\n",
        "            test_texts (list): Test texts\n",
        "\n",
        "        Returns:\n",
        "            tuple: Vectorized training and test texts\n",
        "        \"\"\"\n",
        "        # Fit on training data and transform both\n",
        "        X_train_vectorized = self.tfidf.fit_transform(train_texts)\n",
        "        X_test_vectorized = self.tfidf.transform(test_texts)\n",
        "\n",
        "        return X_train_vectorized, X_test_vectorized\n",
        "\n",
        "class LightweightModels:\n",
        "    @staticmethod\n",
        "    def logistic_regression(X_train, X_test, y_train, y_test):\n",
        "        \"\"\"\n",
        "        Train and evaluate Logistic Regression with memory optimization\n",
        "        \"\"\"\n",
        "        # Use less memory-intensive solver\n",
        "        lr_model = LogisticRegression(\n",
        "            multi_class='ovr',\n",
        "            max_iter=500,\n",
        "            solver='saga',  # More memory-efficient\n",
        "            n_jobs=-1,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Train\n",
        "        lr_model.fit(X_train, y_train)\n",
        "\n",
        "        # Predict\n",
        "        y_pred = lr_model.predict(X_test)\n",
        "\n",
        "        # Report\n",
        "        print(\"\\nLogistic Regression Results:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        return lr_model, y_pred\n",
        "\n",
        "    @staticmethod\n",
        "    def lightweight_neural_network(X_train, X_test, y_train, y_test):\n",
        "        \"\"\"\n",
        "        Train a lightweight neural network with memory optimizations\n",
        "        \"\"\"\n",
        "        # Prepare data\n",
        "        num_classes = len(np.unique(y_train))\n",
        "        y_train_cat = to_categorical(y_train)\n",
        "        y_test_cat = to_categorical(y_test)\n",
        "\n",
        "        # Create lightweight model\n",
        "        model = Sequential([\n",
        "            Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "            Dropout(0.3),\n",
        "            Dense(64, activation='relu'),\n",
        "            Dropout(0.2),\n",
        "            Dense(num_classes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        # Compile with efficient optimizer\n",
        "        model.compile(\n",
        "            optimizer='adam',\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy']\n",
        "        )\n",
        "\n",
        "        # Train with memory-efficient callbacks\n",
        "        history = model.fit(\n",
        "            X_train, y_train_cat,\n",
        "            validation_split=0.2,\n",
        "            epochs=30,\n",
        "            batch_size=64,  # Smaller batch size\n",
        "            callbacks=[\n",
        "                EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "            ],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # Evaluate\n",
        "        _, accuracy = model.evaluate(X_test, y_test_cat, verbose=0)  # Corrected this line\n",
        "        y_pred = np.argmax(model.predict(X_test), axis=1)\n",
        "\n",
        "        print(\"\\nNeural Network Results:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "        return model, y_pred\n",
        "\n",
        "def main():\n",
        "    # Paths to your dataset\n",
        "    train_path = \"/content/Amazon Review Polarity/amazon_review_polarity_csv/train.csv\"\n",
        "    test_path = \"/content/Amazon Review Polarity/amazon_review_polarity_csv/test.csv\"\n",
        "\n",
        "    # Initialize classifier\n",
        "    classifier = AmazonReviewClassifier(\n",
        "        max_features=5000,  # Reduced features\n",
        "        sample_size=50000   # Smaller sample\n",
        "    )\n",
        "\n",
        "    # Load and preprocess data\n",
        "    X_train_texts, X_test_texts, y_train, y_test = classifier.load_and_preprocess_data(\n",
        "        train_path, test_path\n",
        "    )\n",
        "\n",
        "    # Vectorize texts\n",
        "    X_train_vectorized, X_test_vectorized = classifier.vectorize_texts(\n",
        "        X_train_texts, X_test_texts\n",
        "    )\n",
        "\n",
        "    # Convert to dense array with lower precision\n",
        "    X_train_dense = X_train_vectorized.toarray().astype(np.float32)\n",
        "    X_test_dense = X_test_vectorized.toarray().astype(np.float32)\n",
        "\n",
        "    # Train and evaluate models\n",
        "    print(\"\\n--- Logistic Regression ---\")\n",
        "    lr_model, lr_pred = LightweightModels.logistic_regression(\n",
        "        X_train_dense, X_test_dense, y_train, y_test\n",
        "    )\n",
        "\n",
        "    print(\"\\n--- Lightweight Neural Network ---\")\n",
        "    nn_model, nn_pred = LightweightModels.lightweight_neural_network(\n",
        "        X_train_dense, X_test_dense, y_train, y_test\n",
        "    )\n",
        "\n",
        "    # Clear memory\n",
        "    del X_train_texts, X_test_texts, X_train_vectorized, X_test_vectorized\n",
        "    gc.collect()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYAwyQZe4ni6",
        "outputId": "9aa3be63-07aa-46a8-ab69-32b78745da37"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Logistic Regression ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Logistic Regression Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.84      0.84      4972\n",
            "           1       0.84      0.85      0.85      5028\n",
            "\n",
            "    accuracy                           0.85     10000\n",
            "   macro avg       0.85      0.85      0.85     10000\n",
            "weighted avg       0.85      0.85      0.85     10000\n",
            "\n",
            "\n",
            "--- Lightweight Neural Network ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
            "\n",
            "Neural Network Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.85      0.84      4972\n",
            "           1       0.85      0.83      0.84      5028\n",
            "\n",
            "    accuracy                           0.84     10000\n",
            "   macro avg       0.84      0.84      0.84     10000\n",
            "weighted avg       0.84      0.84      0.84     10000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}